{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KrxgSb5T_C9",
        "outputId": "aceca066-6740-43ef-8835-ff83daecd3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Original Dataset Size: 772\n",
            "Balanced Dataset Size: 18\n",
            "Class Distribution in Balanced Data:\n",
            "Class\n",
            "0    9\n",
            "1    9\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "Starting Model Evaluation...\n",
            "Running Simple Random Sampling...\n",
            "Running Systematic Sampling...\n",
            "Running Stratified Sampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2212552980.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n_per_class, random_state=42) if len(x) >= n_per_class else x.sample(len(x), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Cluster Sampling...\n",
            "Running Bootstrap Sampling...\n",
            "\n",
            "==================================================\n",
            "FINAL ACCURACY TABLE\n",
            "==================================================\n",
            "                          Simple Random  Systematic  Stratified  Cluster  \\\n",
            "M1 (Logistic Regression)           0.75        0.50        0.50     0.50   \n",
            "M2 (Decision Tree)                 0.50        1.00        0.75     0.25   \n",
            "M3 (Random Forest)                 0.50        0.75        0.50     0.25   \n",
            "M4 (SVM)                           0.75        0.50        0.75     0.50   \n",
            "M5 (KNN)                           0.75        0.50        0.25     0.50   \n",
            "\n",
            "                          Bootstrap  \n",
            "M1 (Logistic Regression)        1.0  \n",
            "M2 (Decision Tree)              1.0  \n",
            "M3 (Random Forest)              1.0  \n",
            "M4 (SVM)                        0.5  \n",
            "M5 (KNN)                        0.5  \n",
            "\n",
            "==================================================\n",
            "BEST SAMPLING TECHNIQUE PER MODEL\n",
            "==================================================\n",
            "M1 (Logistic Regression): Highest Accuracy (1.0000) with Bootstrap\n",
            "M2 (Decision Tree): Highest Accuracy (1.0000) with Systematic\n",
            "M3 (Random Forest): Highest Accuracy (1.0000) with Bootstrap\n",
            "M4 (SVM): Highest Accuracy (0.7500) with Simple Random\n",
            "M5 (KNN): Highest Accuracy (0.7500) with Simple Random\n",
            "\n",
            "Execution Complete.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. LOAD DATASET\n",
        "# ---------------------------------------------------------\n",
        "# Using the raw URL to download directly into pandas\n",
        "url = \"https://raw.githubusercontent.com/AnjulaMehto/Sampling_Assignment/main/Creditcard_data.csv\"\n",
        "try:\n",
        "    data = pd.read_csv(url)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except:\n",
        "    print(\"Error loading data. Please ensure the URL is correct or upload the CSV manually.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. BALANCE THE DATASET\n",
        "# ---------------------------------------------------------\n",
        "# The dataset is highly imbalanced. We need to balance it as per instructions.\n",
        "# We will undersample the majority class (0) to match the minority class (1).\n",
        "\n",
        "class_0 = data[data['Class'] == 0]\n",
        "class_1 = data[data['Class'] == 1]\n",
        "\n",
        "# Undersample Class 0\n",
        "class_0_balanced = class_0.sample(n=len(class_1), random_state=42)\n",
        "\n",
        "# Concatenate to create balanced dataset\n",
        "balanced_df = pd.concat([class_0_balanced, class_1]).reset_index(drop=True)\n",
        "\n",
        "print(f\"Original Dataset Size: {len(data)}\")\n",
        "print(f\"Balanced Dataset Size: {len(balanced_df)}\")\n",
        "print(f\"Class Distribution in Balanced Data:\\n{balanced_df['Class'].value_counts()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. DEFINE SAMPLING TECHNIQUES\n",
        "# ---------------------------------------------------------\n",
        "# We need 5 different sampling techniques.\n",
        "# NOTE: Formulas for sample size (n) can vary. We will use the Slovin's formula\n",
        "# or standard heuristic sizes for demonstration to ensure code runs easily.\n",
        "\n",
        "# Formula for sample size (Slovin's): n = N / (1 + N*e^2)\n",
        "# Calculating a generic sample size 'n' to use for the samples\n",
        "N = len(balanced_df)\n",
        "e = 0.05 # Margin of error\n",
        "n_slovin = int(N / (1 + N * (e**2)))\n",
        "\n",
        "def sampling_1_simple_random(df, n):\n",
        "    \"\"\"Simple Random Sampling\"\"\"\n",
        "    return df.sample(n=n, random_state=42)\n",
        "\n",
        "def sampling_2_systematic(df, n):\n",
        "    \"\"\"Systematic Sampling\"\"\"\n",
        "    step = len(df) // n\n",
        "    indices = np.arange(0, len(df), step)[:n]\n",
        "    return df.iloc[indices]\n",
        "\n",
        "def sampling_3_stratified(df, n):\n",
        "    \"\"\"Stratified Sampling (maintaining class ratio)\"\"\"\n",
        "    # Ensure n is even for splitting between two classes\n",
        "    n_per_class = int(n/2) if n % 2 == 0 else int(n/2) + (n % 2)\n",
        "    return df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n_per_class, random_state=42) if len(x) >= n_per_class else x.sample(len(x), random_state=42))\n",
        "\n",
        "def sampling_4_cluster(df, n):\n",
        "    \"\"\"Cluster Sampling\"\"\"\n",
        "    # Create fake clusters for demonstration\n",
        "    df_temp = df.copy()\n",
        "    num_clusters = 5\n",
        "    df_temp['cluster'] = np.random.randint(0, num_clusters, size=len(df))\n",
        "\n",
        "    # Select random clusters until we reach rough sample size\n",
        "    # To ensure we get a sample, we might select more than one cluster if the first is too small\n",
        "    sample_size_achieved = 0\n",
        "    selected_clusters = pd.DataFrame()\n",
        "    cluster_ids = np.random.permutation(num_clusters) # Randomize cluster selection order\n",
        "\n",
        "    for cluster_id in cluster_ids:\n",
        "        current_cluster_sample = df_temp[df_temp['cluster'] == cluster_id]\n",
        "        selected_clusters = pd.concat([selected_clusters, current_cluster_sample])\n",
        "        sample_size_achieved = len(selected_clusters)\n",
        "        if sample_size_achieved >= n:\n",
        "            break\n",
        "\n",
        "    # If after selecting all clusters, still less than n, just return all selected\n",
        "    if len(selected_clusters) == 0:\n",
        "        # Fallback if no clusters were large enough to contribute a sample (very unlikely)\n",
        "        return df.sample(n=n, random_state=42) # Revert to simple random if cluster fails\n",
        "\n",
        "    return selected_clusters.head(n).drop(columns=['cluster'])\n",
        "\n",
        "def sampling_5_bootstrap(df, n):\n",
        "    \"\"\"Bootstrap Sampling (Sampling with replacement)\"\"\"\n",
        "    return df.sample(n=n, replace=True, random_state=42)\n",
        "\n",
        "# Dictionary of sampling functions\n",
        "sampling_techniques = {\n",
        "    \"Simple Random\": lambda df: sampling_1_simple_random(df, n_slovin),\n",
        "    \"Systematic\": lambda df: sampling_2_systematic(df, n_slovin),\n",
        "    \"Stratified\": lambda df: sampling_3_stratified(df, n_slovin),\n",
        "    \"Cluster\": lambda df: sampling_4_cluster(df, n_slovin),\n",
        "    \"Bootstrap\": lambda df: sampling_5_bootstrap(df, n_slovin)\n",
        "}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. DEFINE MODELS\n",
        "# ---------------------------------------------------------\n",
        "models = {\n",
        "    \"M1 (Logistic Regression)\": LogisticRegression(max_iter=1000),\n",
        "    \"M2 (Decision Tree)\": DecisionTreeClassifier(random_state=42),\n",
        "    \"M3 (Random Forest)\": RandomForestClassifier(random_state=42),\n",
        "    \"M4 (SVM)\": SVC(probability=True, random_state=42), # Added probability=True and random_state for consistency\n",
        "    \"M5 (KNN)\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. EXECUTE AND EVALUATE\n",
        "# ---------------------------------------------------------\n",
        "results = {}\n",
        "\n",
        "print(\"Starting Model Evaluation...\")\n",
        "\n",
        "for method_name, sampler in sampling_techniques.items():\n",
        "    print(f\"Running {method_name} Sampling...\")\n",
        "\n",
        "    # Generate Sample\n",
        "    sample_df = sampler(balanced_df)\n",
        "\n",
        "    # Handle cases where sample_df might be empty or too small for train_test_split\n",
        "    if len(sample_df) < 2: # Need at least 2 samples for split\n",
        "        print(f\"Skipping {method_name} due to insufficient sample size ({len(sample_df)}).\")\n",
        "        method_accuracies = {model_name: np.nan for model_name in models.keys()}\n",
        "        results[method_name] = method_accuracies\n",
        "        continue\n",
        "\n",
        "    # Split features and target\n",
        "    X = sample_df.drop('Class', axis=1)\n",
        "    y = sample_df['Class']\n",
        "\n",
        "    # Ensure there are enough unique classes in y for stratification if used by train_test_split\n",
        "    if len(y.unique()) < 2: # Need at least two classes for classification\n",
        "        print(f\"Skipping {method_name} due to insufficient classes in sample.\")\n",
        "        method_accuracies = {model_name: np.nan for model_name in models.keys()}\n",
        "        results[method_name] = method_accuracies\n",
        "        continue\n",
        "\n",
        "    # Train/Test Split\n",
        "    # Adjust test_size if sample is too small to ensure both train/test sets are not empty\n",
        "    test_size_val = 0.2\n",
        "    if len(sample_df) * test_size_val < 1: # If test set would be less than 1, adjust\n",
        "        test_size_val = 0.5 if len(sample_df) > 1 else 0.0\n",
        "\n",
        "    # Use stratify=y if possible to maintain class distribution\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_val, random_state=42, stratify=y)\n",
        "    except ValueError as e:\n",
        "        print(f\"Warning: Could not stratify split for {method_name}: {e}. Trying without stratification.\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_val, random_state=42)\n",
        "\n",
        "    # Store accuracy for this sampling method\n",
        "    method_accuracies = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        try:\n",
        "            # Ensure train and test sets are not empty before fitting and predicting\n",
        "            if len(X_train) == 0 or len(X_test) == 0:\n",
        "                method_accuracies[model_name] = np.nan\n",
        "                print(f\"Warning: Not enough data for training/testing with {method_name} and {model_name}.\")\n",
        "                continue\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            method_accuracies[model_name] = acc\n",
        "        except Exception as e:\n",
        "            method_accuracies[model_name] = np.nan # Store as NaN instead of string\n",
        "\n",
        "    results[method_name] = method_accuracies\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. OUTPUT RESULTS\n",
        "# ---------------------------------------------------------\n",
        "# Convert results to DataFrame for the final table\n",
        "final_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL ACCURACY TABLE\")\n",
        "print(\"=\"*50)\n",
        "print(final_df)\n",
        "\n",
        "# Determine which technique gave the highest accuracy for each model\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BEST SAMPLING TECHNIQUE PER MODEL\")\n",
        "print(\"=\"*50)\n",
        "for model_name in models.keys(): # Renamed 'model' to 'model_name' to avoid conflict with the model object\n",
        "    # Convert the row to numeric, coercing errors to NaN\n",
        "    model_results = pd.to_numeric(final_df.loc[model_name], errors='coerce')\n",
        "\n",
        "    if not model_results.isnull().all(): # Check if there are any valid numeric results\n",
        "        best_technique = model_results.idxmax()\n",
        "        best_acc = model_results.max()\n",
        "        print(f\"{model_name}: Highest Accuracy ({best_acc:.4f}) with {best_technique}\")\n",
        "    else:\n",
        "        print(f\"{model_name}: No valid accuracy results to determine best technique.\")\n",
        "\n",
        "print(\"\\nExecution Complete.\")"
      ]
    }
  ]
}